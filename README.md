# Strands Agents ‚Äì Agent Initialization Parameters (Full README)

This README includes all explanations, code, raw notes, and outputs extracted directly from the Jupyter notebook.

# Strands Agents ‚Äì Agent Initialization Parameters

This notebook explains the 15 key parameters for initializing an agent in the Strands SDK, with examples.

See [Strands Documentation](https://strandsagents.com/latest/documentation/docs/) for more.

## 1. `model`
Specifies which LLM backend the agent should use. Can be a string or model object.

```python
import os
from dotenv import load_dotenv
from strands import Agent
from strands.models.litellm import LiteLLMModel

load_dotenv()
gemini_model = LiteLLMModel(
    model_id="gemini/gemini-2.5-pro",
    params={
        "api_key": os.environ.get("GEMINI_API_KEY"),
        "max_tokens": 11000000,
        "temperature": 0.2
    }
)

```

```python

# To run this, you will first need to install the litellm optional dependency:
# pip install 'strands-agents[litellm]'
agent = Agent(
    model=gemini_model,
    system_prompt="You are a precise assistant."
)
# ask agent who is the president of the united states
response = agent("Who is the president of the United States?")
print(response)
```

## 2. `system_prompt`
Defines the agent‚Äôs core instructions/personality.

```python
agent = Agent(
    model=gemini_model,
    system_prompt="You are a legal advisor. Always answer with references to laws."
)
response = agent("Can a person be arrested without a warrant?")
print(response)
```

## 3. `messages`
Preloads a chat history at startup.

```python
agent = Agent(
    model=gemini_model,
    system_prompt="You are a tutor.",
    messages=[
        {
            "role": "user",
            "content": [{"type": "text", "text": "who is the president of the united states?"}]
        },
        {
            "role": "assistant",
            "content": [{"type": "text", "text": "The president of the United States is Joe Biden."}]
        }
    ]
)

response = agent("what question did I ask before and what answer did you reply to my question ?")
print(response)
```

## 4. `tools`
Defines which external tools the agent can use.

```
 1. Why the Browser Tool Needs Bedrock AgentCore
	‚Ä¢	strands_tools.browser depends on AWS Bedrock AgentCore SDK.
	‚Ä¢	That SDK isn‚Äôt on PyPI; it‚Äôs distributed as part of the AWS Bedrock Agent runtime (inside AWS environments, like ECS, Lambda, or Bedrock itself).
	‚Ä¢	So: you can‚Äôt use the browser tool purely locally unless you pull in that SDK.

‚∏ª

üîπ 2. AWS Setup for Browser Tool

To use the Browser tool in AWS Strands Agents, you need:

‚úÖ AWS Account with Bedrock + Bedrock AgentCore enabled
‚úÖ IAM Role that allows:
	‚Ä¢	bedrock:InvokeModel (to call LLMs)
	‚Ä¢	bedrock-agent:InvokeAgent (to use tools)
	‚Ä¢	s3:* (if your tool writes/reads objects)
	‚Ä¢	ssm:GetParameter (if credentials are stored in SSM)

‚úÖ Install Bedrock AgentCore in your environment:

If you‚Äôre running in AWS, the SDK is usually pre-installed.
If not, you‚Äôll need to fetch the package from AWS‚Äôs distribution (not PyPI).

‚∏ª

üîπ 3. Using Browser Tool in an Agent

Once Bedrock AgentCore is available, your code looks like this:
```

```python
from strands_tools import calculator,browser

agent = Agent(
    model=gemini_model,
            system_prompt="Answer with facts.",
            tools=[ calculator,browser ]
)
print("Agent initialized with browser and calculator tools.")
response = agent("What is the population of Japan divided by 100?")
print(response)
```

```python
from strands_tools import calculator,http_request

agent = Agent(
    model=gemini_model,
    system_prompt="Answer with facts.",
    tools=[ calculator,http_request ]
)
response = agent("What is the population of Japan divided by 100?")
print(response)
```

## 5. `callback_handler`

Purpose of callback_handler
	‚Ä¢	A callback_handler in Strands controls logging, debugging, and monitoring of the agent‚Äôs execution steps.
	‚Ä¢	It‚Äôs useful when you want visibility into:
	‚Ä¢	What prompts the agent is sending to the LLM,
	‚Ä¢	What responses are being returned,
	‚Ä¢	Which tools are being called,
	‚Ä¢	Errors or intermediate steps.

	‚Ä¢	PrintingCallbackHandler() ‚Üí A built-in handler that prints events to the console.
	‚Ä¢	callback_handler parameter ‚Üí Passed to the Agent so it hooks into execution.
	‚Ä¢	Result: every time the agent runs, you‚Äôll see step-by-step logs in your terminal (LLM calls, tool usage, errors, outputs).

When to Use
	‚Ä¢	During development and debugging (so you can trace what‚Äôs happening inside your agent).
	‚Ä¢	In production, you‚Äôd usually replace it with a more sophisticated handler, like:
	‚Ä¢	Sending logs to a file,
	‚Ä¢	Monitoring via an external system (Datadog, CloudWatch, etc.),
	‚Ä¢	Triggering alerts when errors occur.


```python
from strands_tools import calculator,http_request
from strands.handlers.callback_handler import PrintingCallbackHandler

agent = Agent(
    model=gemini_model,
    callback_handler=PrintingCallbackHandler(),#default
    system_prompt="Answer with facts.",
    tools=[ calculator,http_request ]
)

response = agent("What is the population of Japan divided by 100? use tools [calculator, http_request] for realtime population data. use http_request for websearch")
print(response)
```

## 6. `conversation_manager`

```

In Strands, your agent‚Äôs ‚Äúcontext‚Äù is the running history the model sees each turn: user messages, the agent‚Äôs replies, tool calls/results, and your system prompt. Conversation managers decide which parts of that history to keep so you don‚Äôt blow past the model‚Äôs context window and cost/time budgets.  Ôøº

By default, Agent uses a SlidingWindowConversationManager, which maintains only the most recent portion of the chat.  Ôøº

Sliding window: how it works
	‚Ä¢	Keeps the last N message pairs (a user message + the assistant‚Äôs reply count as one pair). When a new turn arrives, the oldest pair falls out of the window.  Ôøº
	‚Ä¢	This trimming applies to the same message list the agent uses, which includes tool calls/results when you let Strands record them. (You can toggle this with record_direct_tool_call=False.)  Ôøº
	‚Ä¢	The goal is to stay within model context limits and keep recent, relevant dialog, while automatically shrinking history if a turn risks overflow.  
```

```python
from strands import Agent
from strands.agent.conversation_manager import SlidingWindowConversationManager
from strands.models.openai import OpenAIModel  # swap with GeminiModel / BedrockModel if needed
import warnings
import asyncio

import os
from dotenv import load_dotenv
from strands import Agent
from strands.models.litellm import LiteLLMModel

load_dotenv()
gemini_model = LiteLLMModel(
    model_id="gemini/gemini-2.5-pro",
    params={
        "api_key": os.environ.get("GEMINI_API_KEY"),
        "max_tokens": 11000000,
        "temperature": 0.2
    }
)


conv_manager = SlidingWindowConversationManager(window_size=5)

# 3. Create the agent
agent = Agent(
    model=gemini_model,
    system_prompt="You are a friendly chatbot .",
    conversation_manager=conv_manager,
)

print("‚úÖ Agent initialized with sliding window memory!")


```

```python
# 4. Simulate a conversation
print("User: Hello, my name is vardhaman just remember  do not return anything ?")
agent("Hello, my name is vardhaman just remember  do not return anything ?")


```

```python
print("User: I like cricket. Remember that.")
response2 = agent("I like cricket. Remember that.")
```

```python
for i in range(1, 8):
    print(f"User: This is secret number {i}")
    resp = agent(f"This is secret number {i}.")
    
```

```python
# 5. Now test memory trimming
print("User: Do you still remember what sport I like?")
final_response = agent("Do you still remember what sport I like?")
print("Agent:", final_response)
```

```python
# 5. Now test memory trimming
print("User: can you summarize our conversation?")
print("Agent:", final_response)
```

### SummarizingConversationManager

```
Now let‚Äôs see SummarizingConversationManager ‚Äî this one summarizes older history instead of discarding it.

‚∏ª

üîé Concept
	‚Ä¢	SlidingWindow ‚Üí Keeps last N messages only, forgets everything else.
	‚Ä¢	Summarizing ‚Üí Keeps the last few messages plus a running summary of everything older.

This means:
	‚Ä¢	The agent remembers key facts (like ‚ÄúUser likes cricket‚Äù), even after many turns.
	‚Ä¢	Instead of dropping older messages, they get compressed into a short summary string that gets prepended each time.
```

```python
from strands import Agent
from strands.agent.conversation_manager import SlidingWindowConversationManager,SummarizingConversationManager
from strands.models.openai import OpenAIModel  # swap with GeminiModel / BedrockModel if needed
import warnings
import asyncio

import os
from dotenv import load_dotenv
from strands import Agent
from strands.models.litellm import LiteLLMModel

load_dotenv()
gemini_model = LiteLLMModel(
    model_id="gemini/gemini-2.5-pro",
    params={
        "api_key": os.environ.get("GEMINI_API_KEY"),
        "max_tokens": 11000000,
        "temperature": 0.2
    }
)

conv_manager = SummarizingConversationManager(
    summary_ratio=0.5,
    preserve_recent_messages=5,
    summarization_system_prompt=(
        "Summarize user facts, preferences, and goals. "
        "Always preserve names, dates, locations, and commitments and numbers."
    )
)
# 3. Create the agent
agent = Agent(
    model=gemini_model,
    system_prompt="You are a friendly chatbot that remembers what I say",
    conversation_manager=conv_manager,
)

print("‚úÖ Agent initialized with summarizing memory!")



```

```python
# 4. Simulate a conversation
msg="My name is vardhaman , I love to play cricket, just remember do not return anything "
print(f"User: {msg}")
resp=agent(msg)
```

```python
for i in range(8):
    msg=f"secret code is {i} just remember this do not return anything"
    print(f"User: {msg}")
    resp=agent(msg)
```

```python
# 4. Simulate a conversation
msg=" how many number of recent messages you remembered ?"
print(f"User: {msg}")
resp=agent(msg)
```

```python
msg=" but in conv_manager I set limit to recent messages to 5 how come you remembered all messages ? "
print(f"User: {msg}")
resp=agent(msg)
```

## 7. `record_direct_tool_call`

```

üîπ What is record_direct_tool_call?

When you create an Agent, you can call tools directly via agent.tool.tool_name(...).
This flag decides whether those direct tool calls are recorded in the conversation/chat history or not.
	‚Ä¢	True ‚Üí Tool calls are logged as if they happened in the conversation (traceable, visible in logs).
	‚Ä¢	False ‚Üí Tool calls are executed but not logged (invisible in chat history).


üëâ So the rule of thumb:
	‚Ä¢	True = full trace (great for debugging, auditing, or when you want transparency).
	‚Ä¢	False = hide results from history (good for silent background work or when you don‚Äôt want tool chatter cluttering up logs).
```

```python
from strands import Agent
from strands_tools import calculator
from strands.agent.conversation_manager import SlidingWindowConversationManager
from strands.models.openai import OpenAIModel  # swap with GeminiModel / BedrockModel if needed
import warnings
import asyncio

import os
from dotenv import load_dotenv
from strands import Agent
from strands.models.litellm import LiteLLMModel

load_dotenv()
gemini_model = LiteLLMModel(
    model_id="gemini/gemini-2.5-pro",
    params={
        "api_key": os.environ.get("GEMINI_API_KEY"),
        "max_tokens": 11000000,
        "temperature": 0.2
    }
)

# 2. Create the agent
agent = Agent(
    model=gemini_model,
    system_prompt="You are a friendly chatbot.",
)

# --- Case A: record tool calls in history ---
agent_true = Agent(tools=[calculator], model=gemini_model, record_direct_tool_call=True)
agent_true('add 2 + 2')
print("Chat history (True):")
for msg in agent_true.messages:
    print(msg)


# --- Case B: do NOT record tool calls ---
agent_false = Agent(tools=[calculator], model=gemini_model, record_direct_tool_call=False)
agent_false('add 2 + 2')
print("\nChat history (False):")
for msg in agent_false.messages:
    print(msg)
```

```python
agent = Agent(record_direct_tool_call=False)
print("Agent initialized to not record direct tool calls.")
# result = agent.tool.calculator("2+2")  # Won‚Äôt appear in chat history
```

## 8. `load_tools_from_directory`
Auto-imports Python files from ./tools/ folder.

```python
agent = Agent(
    system_prompt="You can use custom tools.",
    load_tools_from_directory=True,
    model=gemini_model
)
print("Agent initialized to load tools from the './tools' directory.")

agent.tool.greet(name="Vardhaman")
agent.tool.weather(city="Bengaluru")
```

## 9. `trace_attributes`

```
trace_attributes option in Strands is for observability + logging. It attaches custom metadata (tags like team, env, request_id, etc.) to every trace (agent run, tool call, message).

Here‚Äôs how you can test and inspect that the trace attributes are actually being recorded:
```

```
Great question üôå ‚Äî on the surface trace_attributes just looks like ‚Äúextra metadata.‚Äù But in practice it‚Äôs super useful for observability, debugging, and operations when you‚Äôre running agents in production.

Here are some real-world use cases:

‚∏ª

üîπ 1. Multi-team Environments

If several teams share the same agent infrastructure:
	‚Ä¢	Add trace_attributes={"team": "data-eng"}
	‚Ä¢	Observability dashboards (Datadog, Grafana, Jaeger, etc.) can filter traces by team, so each team only sees their own runs.

üëâ Example: Your Data Engineering team‚Äôs runs won‚Äôt get mixed up with the Support team‚Äôs agent runs.

‚∏ª

üîπ 2. Environment / Deployment Tracking

Attach environment tags like:

trace_attributes={"env": "staging"}

Now you can:
	‚Ä¢	Separate traces between dev, staging, and prod.
	‚Ä¢	Verify new code behaves in staging before pushing to prod.
	‚Ä¢	Roll up errors by environment (e.g., ‚ÄúOnly staging is failing ‚Üí check new deployment‚Äù).

‚∏ª

üîπ 3. Customer / Request Correlation

Attach IDs that help you correlate logs:

trace_attributes={"customer_id": "12345", "request_id": "abc-xyz"}

	‚Ä¢	Lets you trace one user‚Äôs request across the whole system.
	‚Ä¢	If a customer says ‚Äúmy report failed,‚Äù you can grep by customer_id.
	‚Ä¢	If you‚Äôre using OpenTelemetry, all logs and spans for that request are grouped.

‚∏ª

üîπ 4. A/B Testing & Feature Flags

You can attach experiment tags:

trace_attributes={"experiment": "prompt_v2"}

	‚Ä¢	Compare latency, error rates, or outcomes between prompt versions.
	‚Ä¢	Easily rollback by filtering results.

‚∏ª

üîπ 5. Performance & Cost Monitoring

Suppose you attach:

trace_attributes={"workflow": "find_email", "model": "gemini-flash"}

You can now:
	‚Ä¢	Break down latency and cost by workflow.
	‚Ä¢	Answer questions like ‚ÄúWhich workflow is costing the most tokens?‚Äù
	‚Ä¢	Spot slow workflows in dashboards.

‚∏ª

üîπ 6. Compliance & Auditing

In regulated industries, you might tag:

trace_attributes={"region": "EU", "gdpr": "true"}

	‚Ä¢	Ensures you can prove that EU user data stayed in EU runs.
	‚Ä¢	Helps auditors filter only traces subject to compliance.

‚∏ª

‚ö° TL;DR

trace_attributes = labels for observability.
They make it possible to filter, debug, and analyze agent activity at scale across teams, environments, customers, and workflows.

‚∏ª

```

```python
from strands import Agent
from strands_tools import calculator

agent = Agent(
    system_prompt="Tracing demo.",
    tools=[calculator],
    model=gemini_model,
    trace_attributes={"team": "data-eng", "env": "staging"}
)

# Run something so a trace is produced
result = agent("What is 3 + 3?")
print("Agent result:", result)

# Inspect the messages
print("\n--- Messages ---")
for msg in agent.messages:
    print(msg)

# Inspect the trace attributes (directly on the agent)
print("\n--- Trace attributes ---")
print(agent.trace_attributes)
```

```python
agent = Agent(
    system_prompt="Tracing demo.",
    trace_attributes={"team": "data-eng", "env": "staging"}
)
print("Agent initialized with trace attributes.")
```

## 10. `agent_id`
Unique identifier for the agent instance.

```python
agent = Agent(
    system_prompt="Travel bot.",
    agent_id="travel-agent-01"
)
print("Agent initialized with a unique agent_id.")
```

## 11. `name`
Friendly display name for the agent.

```python
agent = Agent(
    system_prompt="Finance expert.",
    name="FinanceBot"
)
print("Agent initialized with the name 'FinanceBot'.")
```

## 12. `description`
Describes the agent‚Äôs purpose.

```python
agent = Agent(
    system_prompt="You are a compliance advisor.",
    description="Helps check financial documents for compliance."
)
print("Agent initialized with a description.")
```

## 13. `state`

```
üü¢ What is state?

Think of agent.state like a small notebook the agent always carries.
	‚Ä¢	You (the programmer) can write things in it.
	‚Ä¢	The agent can look at it later.
	‚Ä¢	It stays around even when the agent finishes answering one question.

‚∏ª

üü¢ Why do we need it?

Normally, when you chat:
	‚Ä¢	The agent only remembers the conversation messages.
	‚Ä¢	If you want to keep track of a score, settings, or progress, you‚Äôd have to repeat it in text.

With state, you can keep extra memory outside the chat.

‚∏ª

üü¢ Example in plain words
	1.	Start with score = 0

agent = Agent(
    system_prompt="Game assistant.",
    state=AgentState(data={"score": 0})
)

	2.	Update the score when the player earns points:

agent.state.data["score"] += 5
print(agent.state.data["score"])   # now it‚Äôs 5

Even if the agent forgets the old messages, the score is still there.

‚∏ª

üü¢ Real-life simple uses
	‚Ä¢	Game bot ‚Üí keep track of a player‚Äôs score.
	‚Ä¢	Quiz bot ‚Üí remember how many answers the user got right.
	‚Ä¢	Shopping bot ‚Üí remember items added to the cart.
	‚Ä¢	Preference bot ‚Üí remember user wants answers in ‚Äúshort form‚Äù or ‚ÄúKannada language‚Äù.

‚∏ª

üëâ In one line:
state is like the agent‚Äôs little pocket diary where you can store numbers, flags, or settings between runs.

```

```python
from strands import Agent

# Initialize agent with some state
initial = {
    "user_preferences": {"theme": "dark"},
    "session_count": 0
}
agent = Agent(state=initial,model=gemini_model)

# Access state
print("Theme:", agent.state.get("user_preferences"))        # {'theme': 'dark'}

# Update state
agent.state.set("last_action", "login")
agent.state.set("session_count", agent.state.get("session_count") + 1)

print("Session count:", agent.state.get("session_count"))    # 1
print("Last action:", agent.state.get("last_action"))        # 'login'
```

## 14. `hooks`

```
Got it üëç ‚Äî here‚Äôs a purely detailed explanation, no code, so you can keep it as notes.

‚∏ª

üìù Strands Hooks ‚Äî Detailed Explanation

‚∏ª

üîπ What Hooks Are

Hooks in Strands are like event listeners that let you run custom logic at important steps in the agent‚Äôs lifecycle.
They don‚Äôt change the agent‚Äôs reasoning itself, but they let you observe, log, validate, or extend what‚Äôs happening inside.

Think of them as ‚Äúinterceptors‚Äù ‚Äî they trigger before, during, or after specific events.

‚∏ª

üîπ How Hooks Work
	‚Ä¢	Hooks are organized around events (for example, ‚Äúbefore invocation‚Äù or ‚Äúafter message added‚Äù).
	‚Ä¢	To use hooks, you provide a HookProvider to the agent.
	‚Ä¢	Inside the provider, you say: ‚Äúwhen event X happens, call my function Y.‚Äù
	‚Ä¢	Each event object gives you information about what‚Äôs happening (messages, agent state, etc.).

‚∏ª

üîπ Main Events in Your Version

When we checked your installed version, the following events are available:
	1.	AgentInitializedEvent
	‚Ä¢	Fires when the agent is created.
	‚Ä¢	Useful for logging startup, loading configs, setting up state.
	2.	BeforeInvocationEvent
	‚Ä¢	Fires right before the agent processes a user input.
	‚Ä¢	Lets you inspect the last message or block an invalid input.
	3.	AfterInvocationEvent
	‚Ä¢	Fires after the agent finishes its reasoning for a turn.
	‚Ä¢	Lets you capture outputs, update logs, or save results in a database.
	4.	MessageAddedEvent
	‚Ä¢	Fires whenever a new message is added (from user, assistant, or tool).
	‚Ä¢	This is the most powerful in your version ‚Äî it gives you visibility into:
	‚Ä¢	User prompts
	‚Ä¢	Assistant text replies
	‚Ä¢	Tool calls
	‚Ä¢	Tool results (success or error)

‚∏ª

üîπ How Tool Calls Show Up

Your version doesn‚Äôt have separate tool events. Instead, tool activity appears as special messages:
	‚Ä¢	Tool call ‚Üí Assistant emits a message with a toolUse block.
Example: ‚ÄúAgent is about to call the calculator tool.‚Äù
	‚Ä¢	Tool result (success) ‚Üí User role emits a toolResult with "status": "success".
Example: ‚ÄúCalculator returned 42.‚Äù
	‚Ä¢	Tool result (error) ‚Üí Same structure, but with "status": "error".
Example: ‚ÄúError: division by zero.‚Äù

By watching MessageAddedEvent, you can detect tool start, tool result, and whether it succeeded or failed.

‚∏ª

üîπ The Hook Lifecycle During a Run

Let‚Äôs walk through what happens if you ask the agent ‚ÄúWhat is 5 * 6?‚Äù:
	1.	AgentInitializedEvent
	‚Ä¢	Fired once, when you first create the agent.
	2.	MessageAddedEvent (user)
	‚Ä¢	User message ‚ÄúWhat is 5 * 6?‚Äù is added.
	3.	BeforeInvocationEvent
	‚Ä¢	Agent is about to process that user input.
	4.	MessageAddedEvent (assistant, toolUse)
	‚Ä¢	Assistant decides to call the calculator tool.
	‚Ä¢	Tool name and input appear in the event.
	5.	MessageAddedEvent (user, toolResult)
	‚Ä¢	Calculator returns the result ‚Üí status = success.
	‚Ä¢	If it failed, status = error.
	6.	MessageAddedEvent (assistant, text)
	‚Ä¢	Assistant produces the final text answer: ‚ÄúOK. 5 * 6 = 30.‚Äù
	7.	AfterInvocationEvent
	‚Ä¢	Fired at the very end of the cycle, when the reply is complete.

‚∏ª

üîπ What Hooks Are Useful For
	1.	Observability / Logging
	‚Ä¢	Record every input, output, and tool call for later analysis.
	‚Ä¢	Capture how often each tool is used.
	2.	Error Tracking
	‚Ä¢	Detect when a tool result has "status": "error".
	‚Ä¢	Alert or log failures without stopping the agent.
	3.	Metrics
	‚Ä¢	Count how many runs the agent has processed.
	‚Ä¢	Count tool successes vs errors.
	‚Ä¢	Measure latency (e.g., time between toolUse and toolResult).
	4.	Auditing / Compliance
	‚Ä¢	Keep a trace of what the user asked, what tools were invoked, and what the system replied.
	‚Ä¢	Useful for regulated environments.
	5.	Customization
	‚Ä¢	You can enforce rules (e.g., block certain inputs before invocation).
	‚Ä¢	Modify how results are handled after invocation.
	‚Ä¢	Add custom logging formats for integration with monitoring systems.

‚∏ª

üîπ Key Differences Between Versions
	‚Ä¢	Your version: only has general lifecycle events + message-added events. Tools must be tracked through messages.
	‚Ä¢	Newer versions: also have dedicated tool events (BeforeToolInvocationEvent, AfterToolInvocationEvent) and model-level events (BeforeModelInvocationEvent, AfterModelInvocationEvent).

‚∏ª

üîπ Mental Model
	‚Ä¢	AgentInitializedEvent ‚Üí setup / startup
	‚Ä¢	BeforeInvocationEvent ‚Üí input validation / pre-processing
	‚Ä¢	MessageAddedEvent ‚Üí observe user input, assistant replies, tool calls, tool results (success or error)
	‚Ä¢	AfterInvocationEvent ‚Üí wrap-up, logging, persistence

‚∏ª

‚úÖ In short: Hooks = observability, control, and customization points.
They give you insight into the ‚Äúinside‚Äù of the agent loop and let you log, measure, and react to what‚Äôs happening.
```

```python
from strands import Agent
from strands_tools import calculator
from strands.hooks import HookProvider
from strands.hooks.registry import HookRegistry
from strands.hooks.events import (
    AgentInitializedEvent,
    BeforeInvocationEvent,
    AfterInvocationEvent,
    MessageAddedEvent,
)

class GeneralHooks(HookProvider):
    def register_hooks(self, registry: HookRegistry) -> None:
        registry.add_callback(AgentInitializedEvent, self.on_init)
        registry.add_callback(BeforeInvocationEvent, self.before_invocation)
        registry.add_callback(AfterInvocationEvent, self.after_invocation)
        registry.add_callback(MessageAddedEvent, self.on_message)

    # ---- Lifecycle hooks ----
    def on_init(self, event: AgentInitializedEvent):
        print("[HOOK] Agent initialized")

    def before_invocation(self, event: BeforeInvocationEvent):
        last_msg = event.agent.messages[-1] if event.agent.messages else None
        print(f"[HOOK] Before invocation ‚Üí last message: {last_msg}")

    def after_invocation(self, event: AfterInvocationEvent):
        last_msg = event.agent.messages[-1] if event.agent.messages else None
        print(f"[HOOK] After invocation ‚Üí final message: {last_msg}")

    # ---- Message hook (handles tools & replies) ----
    def on_message(self, event: MessageAddedEvent):
        msg = event.message   # dict
        role = msg.get("role")
        content = msg.get("content")

        if role == "user":
            print(f"[HOOK] User said: {content}")

        elif role == "assistant" and isinstance(content, list):
            block = content[0]

            # Tool call
            if "toolUse" in block:
                tool_use = block["toolUse"]
                print(f"[HOOK] Tool call ‚Üí {tool_use['name']} with input {tool_use['input']}")

            # Tool result (success or error)
            elif "toolResult" in block:
                tool_result = block["toolResult"]
                status = tool_result.get("status")
                if status == "success":
                    print(f"[HOOK] Tool returned SUCCESS ‚Üí {tool_result}")
                elif status == "error":
                    print(f"[HOOK] Tool returned ERROR ‚Üí {tool_result}")

            # Normal assistant text
            elif "text" in block:
                print(f"[HOOK] Assistant replied ‚Üí {block['text']}")

            else:
                print(f"[HOOK] Assistant message (unrecognized): {content}")

# ---------------------------------------
# Agent with generalized hooks
# ---------------------------------------
agent = Agent(
    system_prompt="Demo agent with generalized hooks",
    model=gemini_model,
    tools=[calculator],
    hooks=[GeneralHooks()]
)

print("\n--- Running successful tool call ---")
agent("What is 5 * 6?")

print("\n--- Running tool error case ---")
agent("What is 1 / 0?")   # calculator will error here
```

## 15. `session_manager`

```
Here‚Äôs the ‚Äúsession_manager‚Äù idea in plain, practical terms so you can use it confidently.

What a session manager is

Think of the session manager as the agent‚Äôs filing cabinet.
Each session_id is a separate folder in that cabinet. The manager‚Äôs job is to:
	‚Ä¢	create a folder the first time it sees a new session_id,
	‚Ä¢	keep the conversation history (and optionally other per-session data) in that folder,
	‚Ä¢	pull the right folder before the agent replies,
	‚Ä¢	put the updated folder back after the reply.

With a session manager, multiple users‚Äîor multiple parallel conversations from the same user‚Äîdon‚Äôt mix.

Why you need it
	‚Ä¢	Multi-user apps (web/chatbots/Slack): each user gets their own conversation thread.
	‚Ä¢	Parallel tasks: you can run ‚Äúshopping-assistant:userA‚Äù and ‚Äúreturns-assistant:userA‚Äù at the same time without the messages leaking into each other.
	‚Ä¢	Long-lived context: you can come back hours later with the same session_id and the agent remembers.

What ‚ÄúDefaultSessionManager‚Äù does
	‚Ä¢	It‚Äôs a simple, in-memory implementation.
	‚Ä¢	It keeps a map like: session_id ‚Üí list of messages (+ metadata).
	‚Ä¢	It survives across calls inside the same Python process.
	‚Ä¢	If the process restarts, the memory is gone (i.e., no persistence).
	‚Ä¢	Good for local notebooks, demos, unit tests; not ideal for production or multiple workers.

How sessions change behavior
	‚Ä¢	Without a session_id: all calls share one implicit conversation (easy to step on each other).
	‚Ä¢	With a session_id: each call is routed to that session‚Äôs own history.
‚Äúuser1‚Äù and ‚Äúuser2‚Äù can ask the same question and get answers tailored to their own prior context.

What exactly gets stored per session

Minimum: the message history (user messages, assistant replies, toolUse/toolResult blocks).
Depending on your setup, the manager can also store:
	‚Ä¢	per-session ‚Äústate‚Äù (e.g., a cart, a running score, workflow progress),
	‚Ä¢	lightweight metadata (timestamps, model used, last tool called),
	‚Ä¢	housekeeping (token counters, last access time for TTL cleanup).

How this differs from other knobs
	‚Ä¢	state: mutable variables you set for logic (counters, flags, preferences). Can be tied to a session or global; the session manager is where you persist it per session_id.
	‚Ä¢	messages/chat history: the raw transcript. Session manager organizes this by session.
	‚Ä¢	trace_attributes: tags for observability (team/env/request_id). They help search & monitor, but don‚Äôt store conversation content.
	‚Ä¢	hooks: event callbacks; you can use them to augment the session (e.g., log every run, enforce limits) but they aren‚Äôt storage.

Production patterns (what teams usually do)
	‚Ä¢	Swap the default for a persistent session manager backed by:
	‚Ä¢	Redis (fast, TTL eviction, great for many short sessions),
	‚Ä¢	Postgres/RDS (auditable, queryable, durable),
	‚Ä¢	S3/Blob (cheap long-term transcripts),
	‚Ä¢	or a hybrid (Redis cache + DB archive).
	‚Ä¢	Add TTL & eviction rules so idle folders are cleaned up automatically.
	‚Ä¢	Encrypt PII and restrict cross-session access (don‚Äôt let one user‚Äôs folder be read by another).
	‚Ä¢	Version your stored structures (in case you later change message format).

Common gotchas (and fixes)
	‚Ä¢	Mixing prompts: changing the system prompt mid-session can confuse the model. Start a new session_id if you change core behavior.
	‚Ä¢	ID collisions: generate robust session_ids (UUIDs, or userID+channelID). Don‚Äôt reuse the same ID across different products/roles.
	‚Ä¢	Memory growth: long chats get big. Use a conversation manager (sliding window/summarizer) alongside the session manager so the stored transcript stays model-friendly.
	‚Ä¢	Multi-process apps: the default in-memory manager won‚Äôt share across workers. Use Redis/DB so all app instances see the same sessions.

When to create a new session vs reuse
	‚Ä¢	Reuse when it‚Äôs the same ongoing task for the same person (shopping cart, ticket thread).
	‚Ä¢	New session when the purpose changes (new project, role switch), after a long idle period, or when you want a clean slate to avoid old context bias.

How to think about it with your workflows
	‚Ä¢	Customer support: session_id = supportTicketId. The bot stays scoped to that ticket‚Äôs history.
	‚Ä¢	Data/AI pipelines: session_id = jobRunId. The agent remembers earlier steps and artifacts for the run.
	‚Ä¢	Land/real-estate lead intake: session_id = leadId. The bot collects documents over multiple visits without mixing leads.
	‚Ä¢	Upwork/micro-SaaS demos: keep each demo room separate so investors/clients don‚Äôt see cross-talk.

Quick checklist
	‚Ä¢	Decide your session_id strategy (what uniquely identifies a conversation).
	‚Ä¢	Pick a session backend (memory for dev; Redis/DB for prod).
	‚Ä¢	Set retention/TTL and max history length (pair with a summarizer if needed).
	‚Ä¢	Log who/what/when (for audits) but avoid storing secrets in clear text.
	‚Ä¢	Test concurrency: multiple calls with the same session_id shouldn‚Äôt race or corrupt the folder.

```

```python
from strands.session import DefaultSessionManager

agent = Agent(
    system_prompt="Shopping assistant.",
    session_manager=DefaultSessionManager()
)
print("Agent initialized with a default session manager.")
# Separate sessions
# response1 = agent("I want shoes", session_id="user1")
# response2 = agent("I want laptops", session_id="user2")
# print(f"User 1: {response1}")
# print(f"User 2: {response2}")
```

```python
import asyncio
import asyncpg
import nest_asyncio
import json
from datetime import datetime
from dataclasses import asdict
from strands.session.repository_session_manager import RepositorySessionManager
from strands.session.session_repository import SessionRepository
from strands.types.session import Session, SessionAgent, SessionMessage
from strands_tools import calculator
nest_asyncio.apply()

def run_sync(coro):
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        return asyncio.run(coro)
    return loop.run_until_complete(coro)


class PostgresRepository(SessionRepository):
    def __init__(self, dsn):
        self.dsn = dsn

    async def _get_conn(self):
        return await asyncpg.connect(self.dsn)

    def _current_ts(self):
        return datetime.utcnow().isoformat()

    # -----------------------
    # Session methods
    # -----------------------
    def create_session(self, session: Session) -> Session:
        data = asdict(session)
        data['created_at'] = self._current_ts()
        data['updated_at'] = self._current_ts()
        run_sync(self._exec(
            """
            INSERT INTO sessions_meta (session_id, data)
            VALUES ($1, $2::jsonb)
            ON CONFLICT (session_id) DO UPDATE SET data=$2::jsonb
            """,
            session.session_id,
            json.dumps(data)   # ‚úÖ dict ‚Üí JSON string
        ))
        return session

    def read_session(self, session_id: str) -> Session | None:
        row = run_sync(self._fetchrow(
            "SELECT data FROM sessions_meta WHERE session_id = $1",
            session_id
        ))
        if not row:
            return None
        return Session.from_dict(json.loads(row['data']))  # ‚úÖ JSON string ‚Üí dict

    # -----------------------
    # Agent methods
    # -----------------------
    def create_agent(self, session_id: str, session_agent: SessionAgent) -> None:
        data = asdict(session_agent)
        data['created_at'] = self._current_ts()
        data['updated_at'] = self._current_ts()
        run_sync(self._exec(
            """
            INSERT INTO agents_meta (session_id, agent_id, data)
            VALUES ($1, $2, $3::jsonb)
            ON CONFLICT (session_id, agent_id) DO UPDATE SET data=$3::jsonb
            """,
            session_id,
            session_agent.agent_id,
            json.dumps(data)   # ‚úÖ dict ‚Üí JSON string
        ))

    def read_agent(self, session_id: str, agent_id: str) -> SessionAgent | None:
        row = run_sync(self._fetchrow(
            "SELECT data FROM agents_meta WHERE session_id=$1 AND agent_id=$2",
            session_id, agent_id
        ))
        return SessionAgent.from_dict(json.loads(row['data'])) if row else None

    def update_agent(self, session_id: str, session_agent: SessionAgent) -> None:
        data = asdict(session_agent)
        data['updated_at'] = self._current_ts()
        run_sync(self._exec(
            "UPDATE agents_meta SET data=$1::jsonb WHERE session_id=$2 AND agent_id=$3",
            json.dumps(data),   # ‚úÖ dict ‚Üí JSON string
            session_id,
            session_agent.agent_id
        ))

    # -----------------------
    # Message methods
    # -----------------------
    def create_message(self, session_id: str, agent_id: str, session_message: SessionMessage) -> None:
        data = asdict(session_message)
        data['created_at'] = self._current_ts()
        run_sync(self._exec(
            """
            INSERT INTO messages (session_id, agent_id, message_id, data)
            VALUES ($1, $2, $3, $4::jsonb)
            ON CONFLICT (session_id, agent_id, message_id) DO UPDATE SET data=$4::jsonb
            """,
            session_id,
            agent_id,
            session_message.message_id,
            json.dumps(data)   # ‚úÖ dict ‚Üí JSON string
        ))

    def read_message(self, session_id: str, agent_id: str, message_id: str) -> SessionMessage | None:
        row = run_sync(self._fetchrow(
            "SELECT data FROM messages WHERE session_id=$1 AND agent_id=$2 AND message_id=$3",
            session_id, agent_id, message_id
        ))
        return SessionMessage.from_dict(json.loads(row['data'])) if row else None

    def update_message(self, session_id: str, agent_id: str, session_message: SessionMessage) -> None:
        data = asdict(session_message)
        data['updated_at'] = self._current_ts()
        run_sync(self._exec(
            "UPDATE messages SET data=$1::jsonb WHERE session_id=$2 AND agent_id=$3 AND message_id=$4",
            json.dumps(data),   # ‚úÖ dict ‚Üí JSON string
            session_id,
            agent_id,
            session_message.message_id
        ))

    def list_messages(self, session_id: str, agent_id: str, limit=None, offset=0) -> list[SessionMessage]:
        rows = run_sync(self._fetch(
            "SELECT data FROM messages WHERE session_id=$1 AND agent_id=$2 ORDER BY message_id ASC LIMIT $3 OFFSET $4",
            session_id, agent_id, limit or 100, offset
        ))
        return [SessionMessage.from_dict(json.loads(r['data'])) for r in rows]

    # -----------------------
    # Internal helpers
    # -----------------------
    async def _exec(self, query, *args):
        conn = await self._get_conn()
        try:
            await conn.execute(query, *args)
        finally:
            await conn.close()

    async def _fetchrow(self, query, *args):
        conn = await self._get_conn()
        try:
            return await conn.fetchrow(query, *args)
        finally:
            await conn.close()

    async def _fetch(self, query, *args):
        conn = await self._get_conn()
        try:
            return await conn.fetch(query, *args)
        finally:
            await conn.close()
```

```python
repo = PostgresRepository("postgresql://vardhamanrparappanavar:password@localhost/postgres")
session_manager = RepositorySessionManager(session_id="varun", session_repository=repo)

agent = Agent(
    system_prompt=" you are help assistant remember about my personal details",
    tools=[calculator],
    model=gemini_model,
    session_manager=session_manager
)

resp1 = agent("My name is vardhaman ", session_id="varun")
resp2 = agent("I like to play football", session_id="varun")
resp3 = agent("I hate golf", session_id="varun")
```

```python
repo = PostgresRepository("postgresql://vardhamanrparappanavar:password@localhost/postgres")
session_manager = RepositorySessionManager(session_id="varun", session_repository=repo)

agent = Agent(
    system_prompt=" you are help assistant",
    tools=[calculator],
    model=gemini_model,
    session_manager=session_manager
)

resp1 = agent("tell me about me ", session_id="varun")
```

## üìä Quick Summary Table

| Param | Purpose | Example Use |
|-------|---------|-------------|
| model | Choose LLM | GPT-4 vs Palmyra-X5 |
| system_prompt | Define role | ‚ÄúLegal assistant‚Äù |
| messages | Preload history | Resume old chat |
| tools | Enable functions | web_search, calc |
| callback_handler | Logging/debug | Print steps |
| conversation_manager | Short-term memory | Keep last 5 msgs |
| record_direct_tool_call | Log tool calls? | Hide/surface |
| load_tools_from_directory | Auto-load tools | ./tools/weather.py |
| trace_attributes | Observability | env=staging |
| agent_id | Unique ID | ‚Äúfinance-01‚Äù |
| name | Display name | ‚ÄúFinanceBot‚Äù |
| description | Metadata | ‚ÄúCompliance checker‚Äù |
| state | Internal memory | {score: 0} |
| hooks | Inject custom logic | Before/after reply |
| session_manager | Long-term sessions | Keep user-specific chats |

